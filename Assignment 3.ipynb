{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Eric Sieu Diep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "(X,y) = load_concrete(data_home=None, return_dataset=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Accuracy Error Validation Accuracy Error\n",
      "DT               47.279761                 73.447331\n",
      "RF                8.288296                  34.40223\n",
      "GB               12.955436                 28.506459\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, random_state = 0)\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state = 0, max_depth=5)\n",
    "forest = RandomForestRegressor(random_state = 0, n_estimators = 5)\n",
    "grbt = GradientBoostingRegressor(random_state = 0)\n",
    "\n",
    "models = [ tree, forest, grbt]\n",
    "results = pd.DataFrame(columns= [\"Training Accuracy Error\", \"Validation Accuracy Error\"], index=[\"DT\", \"RF\", \"GB\"])\n",
    "\n",
    "i = 0;\n",
    "for model in models:\n",
    "    #model.fit(X_train, y_train)\n",
    "    j = 0; \n",
    "    scores = cross_validate(model, X_train, y_train, scoring = 'neg_mean_squared_error', return_train_score = True)\n",
    "    results.iloc[i,j] = scores['train_score'].mean()*(-1)\n",
    "    results.iloc[i, j+1] = scores['test_score'].mean()*(-1)   \n",
    "    i += 1;\n",
    "    \n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83539f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following scores belong to decision tree, random forest and gradient boosting machine regression respectively: \n",
      "Accuracy on training set: 0.832\n",
      "Accuracy on validation set 0.675\n",
      "\n",
      "Accuracy on training set: 0.973\n",
      "Accuracy on validation set 0.865\n",
      "\n",
      "Accuracy on training set: 0.950\n",
      "Accuracy on validation set 0.906\n",
      "\n",
      "   Training Accuracy Error Validation Accuracy Error\n",
      "DT                0.834465                  0.738697\n",
      "RF                0.971015                  0.878148\n",
      "GB                0.954713                  0.899428\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(\"The following scores belong to decision tree, random forest and gradient boosting machine regression respectively: \")\n",
    "i = 0;\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    j = 0;\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "    \n",
    "    scores = cross_validate(model, X_train, y_train, scoring = 'r2', return_train_score = True)\n",
    "    results.iloc[i,j] = scores['train_score'].mean()\n",
    "    print(\"Accuracy on validation set {:.3f}\".format(model.score(X_test, y_test)))\n",
    "    results.iloc[i, j+1] = scores['test_score'].mean()\n",
    "    print(\"\")\n",
    "    i += 1;\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "\n",
    "These results are much better than that of a linear model used in previous assignment. The R2 score achived by a linear model are only 0.61 and 0.62 for training and testing score respectively. In the tree base model, the worst training and testing scores are 0.83 and 0.74 (about 20% better than linear model) while the best scores are 0.95 and 0.9 (about 45% better)\n",
    "\n",
    "2. Out of the models you tested, which model would you select for this dataset and why?\n",
    "\n",
    "According to the the results table, the gradient bossting machine model gives the best score, the random forest are second best and single decision tree ranks bottom. However, the scores achieved from the GB model are only about 2% higher than that of the RF, which is insignificant. Since GB requires careful tuning of the parameters, which can be difficult to do, while RF is tuning-free*, I will choose random forest for this dataset because it is simpler to execute and the results achieve is as good as the best model and about 20% better than a single decision tree.\n",
    "*source: https://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/\n",
    "\n",
    "3. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "To improve the accuracy, we can:\n",
    " - increase the the number of n_estimators as more trees will provide better result with a better average scores. Note that this would take longer\n",
    " - tuning the max_depth of the trees to find an optimized number of depth\n",
    " - tuning the number of features used, as max_features determines how random each tree is, and a smaller max_features reduces overfitting\n",
    " Source: section 2.3 in https://www.keboola.com/blog/random-forest-regression\n",
    " \n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "I found the source code in the example provided in lecture. The lectures slides did an excellent job explaining the concepts and the jupyter notebook example was very helpful. \n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "The assignment has clear instructions and steps that I follow with ease to complete the assignment. Again, the steps reinforces the five steps in Machine Learning Workflow taught in the begining of the class.\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use AI for this assignment.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "I don't have any challaenges completing this assignment as the ideas are about the same as previous assignments. Only difference commands/funciton are used. I still need to look up on the syntax and the command name in order to spell them correctly. I go to the scikit documentation site to check on the function command and its arguments/attributes. I found it's very helpful learning to read the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065\n",
      "1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050\n",
      "1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185\n",
      "1,14.37,1.95,2.5,16.8,113,3.85,3.49,.24,2.18,7.8,.86,3.45,1480\n",
      "1,13.24,2.59,2.87,21,118,2.8,2.69,.39,1.82,4.32,1.04,2.93,735\n",
      "1,14.2,1.76,2.45,15.2,112,3.27,3.39,.34,1.97,6.75,1.05,2.85,1450\n",
      "1,14.39,1.87,2.45,14.6,96,2.5,2.52,.3,1.98,5.25,1.02,3.58,1290\n",
      "1,14.06,2.15,2.61,17.6,121,2.6,2.51,.31,1.25,5.05,1.06,3.58,1295\n",
      "1,14.83,1.64,2.17,14,97,2.8,2.98,.29,1.98,5.2,1.08,2.85,1045\n",
      "1,13.86,1.35,2.27,16,98,2.98,3.15,.22,1.85,7.22,1.01,3.55,1045\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "!head \"C:\\Users\\Eric\\MEng\\MEng-Fall-2023\\ENSF611\\a3\\wine\\wine.data\"\n",
    "data = pd.read_csv(r\"C:\\Users\\Eric\\MEng\\MEng-Fall-2023\\ENSF611\\a3\\wine\\wine.data\", \n",
    "                   names = ['Wine Class', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', 'Magnesium', \n",
    "                           'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                           'Color Intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'],\n",
    "                  header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Wine Class  Alcohol  Malic Acid   Ash  Alcalinity of Ash  Magnesium  \\\n",
      "0           1    14.23        1.71  2.43               15.6        127   \n",
      "1           1    13.20        1.78  2.14               11.2        100   \n",
      "2           1    13.16        2.36  2.67               18.6        101   \n",
      "3           1    14.37        1.95  2.50               16.8        113   \n",
      "4           1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color Intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Wine Class                    178 non-null    int64  \n",
      " 1   Alcohol                       178 non-null    float64\n",
      " 2   Malic Acid                    178 non-null    float64\n",
      " 3   Ash                           178 non-null    float64\n",
      " 4   Alcalinity of Ash             178 non-null    float64\n",
      " 5   Magnesium                     178 non-null    int64  \n",
      " 6   Total phenols                 178 non-null    float64\n",
      " 7   Flavanoids                    178 non-null    float64\n",
      " 8   Nonflavanoid phenols          178 non-null    float64\n",
      " 9   Proanthocyanins               178 non-null    float64\n",
      " 10  Color Intensity               178 non-null    float64\n",
      " 11  Hue                           178 non-null    float64\n",
      " 12  OD280/OD315 of diluted wines  178 non-null    float64\n",
      " 13  Proline                       178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(data.head(5))\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Class                      0\n",
      "Alcohol                         0\n",
      "Malic Acid                      0\n",
      "Ash                             0\n",
      "Alcalinity of Ash               0\n",
      "Magnesium                       0\n",
      "Total phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color Intensity                 0\n",
      "Hue                             0\n",
      "OD280/OD315 of diluted wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37a6fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wine Class\n",
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "data['Wine Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X matrix:  (178, 13)\n",
      "Dimension of target variable:  (178,)\n",
      "(133, 13)\n",
      "(45, 13)\n",
      "    Training Accuraccy Validation Accuracy\n",
      "DTC                1.0            0.901994\n",
      "SVC           0.680427            0.676638\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "#import library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#set up the X, y matrix\n",
    "X = data.drop(columns = 'Wine Class')\n",
    "y = data['Wine Class']\n",
    "print('Dimension of X matrix: ', X.shape)\n",
    "print('Dimension of target variable: ', y.shape)\n",
    "\n",
    "#spliting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_train.value_counts()\n",
    "y_test.value_counts()\n",
    "\n",
    "#instantiate the model\n",
    "tree_classifier = DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
    "svc = SVC(random_state = 0)\n",
    "models = [tree, svc]\n",
    "results = pd.DataFrame(columns = ['Training Accuraccy', 'Validation Accuracy'], index = ['DTC', 'SVC'])\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    j = 0\n",
    "    scores = cross_validate(model,X_train, y_train, scoring = 'accuracy', return_train_score = True)\n",
    "    results.iloc[i, j] = scores['train_score'].mean()\n",
    "    results.iloc[i, j+1] = scores['test_score'].mean()\n",
    "    i += 1\n",
    "    \n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:\n",
    "\n",
    "The decistion tree classifier gave the highest accuracy at 0.9 while the SVC only gave an accuracy of 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement best model\n",
    "best_model = tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d21b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, best_model.predict(X_test))\n",
    "sns.heatmap(mat, square = True, annot = True, cbar = False)\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('true value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.88      0.93        16\n",
      "           2       0.91      0.95      0.93        21\n",
      "           3       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, best_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "The decision tree classifier did very well with a training score of 1.0 and a accuracy score of 0.9. It did excellent in learning the model and predicting the value. On the other hand, the support vector machine models did poorly compared to the DTC. The training score and accuracy score drops drastically over 30% to 0.68 for both score. Note that the variance is extremely low for both models.\n",
    "\n",
    "2. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "\n",
    " - The decision boundary created by the SVM is most likely non-linear.The SVM adds non-linear features in the model, and it is possible that the dataset used is better fit with linear features.\n",
    " - Another possible cause is the data was not preprocessed properly, and the hyper -parameters were not tuned properly. The SVM required the parameters to be tuned carefully for it to work.\n",
    " \n",
    "3. How many samples were incorrectly classified in step 5.2? \n",
    "Three samples were incorrectly classified accoridng to the confusion matrix.\n",
    "\n",
    "4. In this case, is maximizing precision or recall more important? Why?\n",
    "In this case, it doesn't really matter whether maximizing precision or recall. They are equally important, as I don't see why it is important to correctly classify a wine unless there is an economic value. As stated in the source of the data site \"These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\" \n",
    "\n",
    "Precision focuses on false positive i.e. a wine classfied as positive category but it is not (for eg, it is classified as class 1, but belongs to class 2 or 3). Recall focuses on false negative i.e. a wine classified in a negative class, but it is not (for eg it is not classified in class 1, (i.e. classified in class 2 or 3), but it is actually class 1). Assuming class 1 wine is a lot more expensive ( and expensive enough to make an impact) than the other two classes , maximizing precisons means minimize the chances of selling \"fake\" wine ( aka selling a cheap wine at a much higher price), while maximizing recall means minimizing in loss of revenue. The former is reputation and risk of being sued, while the later is money which are equally improtant in my opinion.\n",
    "\n",
    "On a second thought, I won't spend money on wine anyway, so it doesn't matter to me.\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "\n",
    "1.Where did you source your code?\n",
    "\n",
    "I found the source code in the example provided in the jupyter notebook. The coding part is not difficult\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "I follow the order in the Machine Leanring Workflow which is also the steps of the instruction. Loading data, inspect and cleaning data, load the model and instantiate the ML model, split the data into train and test, validate the model and visualize the results\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use AI.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "I found it's a challenge to read and intepret the confusion matrix for multiclass. It's easy to identified TPï¼ŒTN, FP and FN in a binary matrix. I did some google and found a very good website explaining it https://www.evidentlyai.com/classification-metrics/confusion-matrix I have a very good understanding about confustion matrix and its metrics of recall and precision. However, I still have a hard time to decide whether recall or precision is more important as it depends on the actual kind of data model. In this assignment, predicting a wine class, I don't really see the importance whether a type of wine is correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "I notice that it's important to choose the right model for the datasets. A linear models for linear data and vice versa. In the concrete examples, a linear model did very poorly (accuracy score of 0.6, while the non-linear (tree, forest and grbt) did very well (accuracy score between 0.7 to 0.9). The data source mentioned that there are non-linear features existed in the datasets. \n",
    "\n",
    "I also notice that a more complex model doesn't neccessary mean a better result. As in the concrete examples in part 1, random forest gradient boosting model generate about the same result (0.88 vs 0.9). In the wine example, the decision tree is better than the support vector (0.9 vs 0.7 scores) noted that decision tree doesn't required hyper parameter tuning, while the other two do need a careful tuning. Decision tree is considered less powerful than the other two, but it is easier to work with. So always start with a simple model first is the key idea discusse in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "\n",
    "I like the fact that the assignment makes use of various non-linear models and make a comparision among the results of each model. This reinforces what discussed in the lecture about the weakness and strength of each model by seeing the results in a table. Also, the assignment also makes us refer to the results of previous assignment, so we can see the difference between linear an non-linear models, which is great. \n",
    "\n",
    "The confusion matrix is a bit confusing, but I got it figured out after further reading.The meaning of recall and precison still remains confusing as it depends on the actual data. So I will just watch out for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of scores:  <class 'dict'>\n",
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n",
      "train score:  0.9059248809733734\n",
      "test score:  0.8717948717948719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(max_iter = 5000)\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_validate(lsvc, X_train, y_train, return_train_score = True, scoring = 'accuracy')\n",
    "print(\"type of scores: \", type(scores))\n",
    "print(scores.keys())\n",
    "\n",
    "print(\"train score: \", scores['train_score'].mean())\n",
    "print(\"test score: \", scores['test_score'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using LinearSVC improves the result significantly compared to SVC. With Linear SVC the results improved over 20% with a scores \n",
    "closer to 0.9 from 0.7 apprx. As mentioned before, the dataset is better suite with linear features than non linear features. \n",
    "LinearSVC has more linear features in the model that make the decision boundary more linear, hence improving the score to 0.9.\n",
    "This make Linear SVC a good fit for the model with a very low variance between the training score and testing score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
